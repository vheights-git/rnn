# -*- coding: utf-8 -*-
"""Twitter_Disaster_Tweets_Submission_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qVhYPAcc69dsO3ofo5kBZ6yYNUiAhCjw
"""

# Commented out IPython magic to ensure Python compatibility.
# Dependencies
import numpy as np
import pandas as pd
import os
for dirname, _, filenames in os.walk('nlp-getting-started'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
# %matplotlib inline

import warnings
warnings.filterwarnings('ignore')

import seaborn as sns
plt.style.use('ggplot')

# Reading data

train = pd.read_csv('nlp-getting-started/twitter-data/train.csv')
test = pd.read_csv('nlp-getting-started/twitter-data/test.csv')
submission = pd.read_csv('nlp-getting-started/twitter-data/sample_submission.csv')

print('Training data shape = {}'.format(train.shape))
print('Training data memory = {:.2f} MB'.format(train.memory_usage().sum() / 1024**2))
print(f"Training sample:\n{train.sample(5, random_state=42)}", end="\n\n")

print('Test data shape = {}'.format(test.shape))
print('Test data memory usage = {:.2f} MB'.format(test.memory_usage().sum() / 1024**2))
print(f"Test sample:\n{test.sample(5, random_state=42)}", end="\n\n")

print('Submission shape = {}'.format(submission.shape))
print('Submission data memory usage = {:.2f} MB'.format(submission.memory_usage().sum() / 1024**2))
print(f"Submission sample:\n{submission.sample(5, random_state=42)}")

!pip install nltk

!pip install wordcloud

!pip install gensim

# Dependencies

import re
import string

import nltk
nltk.download('punkt_tab')
from nltk.util import ngrams
from nltk.corpus import stopwords
stop=set(stopwords.words('english'))
from nltk.tokenize import word_tokenize

from sklearn.manifold import TSNE
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.preprocessing import LabelEncoder

from collections import defaultdict
from collections import  Counter

from wordcloud import WordCloud
import gensim
import string

from tqdm import tqdm
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout
from tensorflow.keras.initializers import Constant
from tensorflow.keras.optimizers import Adam

# Class distributions in the training dataset

class_distribution = train['target'].value_counts(normalize=True) * 100
print("class distribution (%):")
print(class_distribution)

fig, axes = plt.subplots(ncols=2, figsize=(10, 4), dpi=100)
plt.tight_layout()

train.groupby('target').count()['id'].plot(kind='pie', ax=axes[0], labels=['non-disaster (57%)', 'disaster (43%)'])
sns.countplot(x=train['target'], hue=train['target'], ax=axes[1])

axes[0].set_ylabel('')
axes[1].set_ylabel('')
axes[1].set_xticklabels(['non-disaster (4342)', 'disaster (3271)'])
axes[0].tick_params(axis='x', labelsize=15)
axes[0].tick_params(axis='y', labelsize=15)
axes[1].tick_params(axis='x', labelsize=15)
axes[1].tick_params(axis='y', labelsize=15)

axes[0].set_title('Target distribution in training Set', fontsize=13)
axes[1].set_title('Target count in training Set', fontsize=13)

plt.show()

# Meta features distributions for word count, unique word count, character count, mean word length, punctuation, stop word count, hashtag count, mention count in the training dataset and test dataset

# char count
train['char_count'] = train['text'].apply(lambda x: len(str(x)))
test['char_count'] = test['text'].apply(lambda x: len(str(x)))

# word count
train['word_count'] = train['text'].apply(lambda x: len(str(x).split()))
test['word_count'] = test['text'].apply(lambda x: len(str(x).split()))

# unique word count
train['unique_word_count'] = train['text'].apply(lambda x: len(set(str(x).split())))
test['unique_word_count'] = test['text'].apply(lambda x: len(set(str(x).split())))

# mean word length
train['mean_word_length'] = train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))
test['mean_word_length'] = test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))

# stop word count
train['stop_word_count'] = train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop]))
test['stop_word_count'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop]))

# url count
train['url_count'] = train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))
test['url_count'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))

# punctuation count
train['punctuation_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))
test['punctuation_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))

# hashtag count
train['hashtag_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))
test['hashtag_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))

# mention count
train['mention_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))
test['mention_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))

# Visualizations
meta_features = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',
                'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']
disaster_tweets = train['target'] == 1

fig, axes = plt.subplots(ncols=2, nrows=len(meta_features), figsize=(10, 35), dpi=100)

for i, feature in enumerate(meta_features):
    sns.distplot(train.loc[~disaster_tweets][feature], label='non-disaster', ax=axes[i][0], color='green')
    sns.distplot(train.loc[disaster_tweets][feature], label='disaster', ax=axes[i][0], color='red')

    sns.distplot(train[feature], label='Training', ax=axes[i][1])
    sns.distplot(test[feature], label='Test', ax=axes[i][1])

    for j in range(2):
        axes[i][j].set_xlabel('')
        axes[i][j].tick_params(axis='x', labelsize=12)
        axes[i][j].tick_params(axis='y', labelsize=12)
        axes[i][j].legend()

    axes[i][0].set_title(f'{feature} target distribution in training Set', fontsize=10)
    axes[i][1].set_title(f'{feature} training & test set distribution', fontsize=10)

plt.show()

# Finding missing values in the keyword and location dimensions and putting placeholder values

missing_cols = ['keyword', 'location']

fig, axes = plt.subplots(ncols=2, figsize=(8, 4), dpi=100)

sns.barplot(x=train[missing_cols].isnull().sum().index, y=train[missing_cols].isnull().sum().values, ax=axes[0])
sns.barplot(x=test[missing_cols].isnull().sum().index, y=test[missing_cols].isnull().sum().values, ax=axes[1])

axes[0].set_ylabel('missing value count', size=15, labelpad=20)
axes[0].tick_params(axis='x', labelsize=15)
axes[0].tick_params(axis='y', labelsize=15)
axes[1].tick_params(axis='x', labelsize=15)
axes[1].tick_params(axis='y', labelsize=15)

axes[0].set_title('training set', fontsize=13)
axes[1].set_title('test set', fontsize=13)

plt.show()

# Finding missing values numerically
for name, df in zip(['TRAIN', 'TEST'], [train, test]):
    missing_percent = df[['keyword', 'location']].isna().mean() * 100
    print(f"\nMissing values in {name} (%):")
    print(missing_percent)

# Replacing missing NaN with placeholders
for df in [train, test]:
    for col in ['keyword', 'location']:
        df[col] = df[col].fillna(f'no_{col}')

# Analysing Keywords in training and test dataset

print(f'Number of unique values in keyword = {train["keyword"].nunique()} (Training) - {test["keyword"].nunique()} (Test)')
print(f'Number of unique values in location = {train["location"].nunique()} (Training) - {test["location"].nunique()} (Test)')

# Keywords distribution in target
train['target_mean'] = train.groupby('keyword')['target'].transform('mean')

fig = plt.figure(figsize=(5, 50), dpi=100)

sns.countplot(y=train.sort_values(by='target_mean', ascending=False)['keyword'],
              hue=train.sort_values(by='target_mean', ascending=False)['target'])

plt.tick_params(axis='x', labelsize=8)
plt.tick_params(axis='y', labelsize=8)
plt.legend(loc=1)
plt.title('target distribution in Keywords')

plt.show()

train.drop(columns=['target_mean'], inplace=True)

# Analysing mislabeled samples based on text dimension and relabeling them

mislabeled = train.groupby(['text']).nunique().sort_values(by='target', ascending=False)
mislabeled = mislabeled[mislabeled['target'] > 1]['target']
mislabeled.index.tolist()

train['target_relabeled'] = train['target'].copy()

train.loc[train['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_relabeled'] = 0
train.loc[train['text'] == 'Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife', 'target_relabeled'] = 0
train.loc[train['text'] == 'To fight bioterrorism sir.', 'target_relabeled'] = 0
train.loc[train['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target_relabeled'] = 1
train.loc[train['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target_relabeled'] = 1
train.loc[train['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_relabeled'] = 0
train.loc[train['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_relabeled'] = 0
train.loc[train['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\n \n#FARRAKHAN #QUOTE', 'target_relabeled'] = 1
train.loc[train['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target_relabeled'] = 1
train.loc[train['text'] == "Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...", 'target_relabeled'] = 0
train.loc[train['text'] == "wowo--=== 12000 Nigerian refugees repatriated from Cameroon", 'target_relabeled'] = 0
train.loc[train['text'] == "He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam", 'target_relabeled'] = 0
train.loc[train['text'] == "Hellfire! We donÛªt even want to think about it or mention it so letÛªs not do anything that leads to it #islam!", 'target_relabeled'] = 0
train.loc[train['text'] == "The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'", 'target_relabeled'] = 0
train.loc[train['text'] == "Caution: breathing may be hazardous to your health.", 'target_relabeled'] = 1
train.loc[train['text'] == "I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????", 'target_relabeled'] = 0
train.loc[train['text'] == "#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect", 'target_relabeled'] = 0
train.loc[train['text'] == "that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time", 'target_relabeled'] = 0

# Generating n-grams

def generate_ngrams(text, n_gram=1):
    token = [token for token in text.lower().split(' ') if token != '' if token not in stop]
    ngrams = zip(*[token[i:] for i in range(n_gram)])
    return [' '.join(ngram) for ngram in ngrams]

# Top N items to be shown
N = 20

# Unigrams
disaster_unigrams = defaultdict(int)
nondisaster_unigrams = defaultdict(int)

for tweet in train[disaster_tweets]['text']:
    for word in generate_ngrams(tweet):
        disaster_unigrams[word] += 1

for tweet in train[~disaster_tweets]['text']:
    for word in generate_ngrams(tweet):
        nondisaster_unigrams[word] += 1

df_disaster_unigrams = pd.DataFrame(sorted(disaster_unigrams.items(), key=lambda x: x[1])[::-1])
df_nondisaster_unigrams = pd.DataFrame(sorted(nondisaster_unigrams.items(), key=lambda x: x[1])[::-1])

# Bigrams
disaster_bigrams = defaultdict(int)
nondisaster_bigrams = defaultdict(int)

for tweet in train[disaster_tweets]['text']:
    for word in generate_ngrams(tweet, n_gram=2):
        disaster_bigrams[word] += 1

for tweet in train[~disaster_tweets]['text']:
    for word in generate_ngrams(tweet, n_gram=2):
        nondisaster_bigrams[word] += 1

df_disaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1])
df_nondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1])

# Trigrams
disaster_trigrams = defaultdict(int)
nondisaster_trigrams = defaultdict(int)

for tweet in train[disaster_tweets]['text']:
    for word in generate_ngrams(tweet, n_gram=3):
        disaster_trigrams[word] += 1

for tweet in train[~disaster_tweets]['text']:
    for word in generate_ngrams(tweet, n_gram=3):
        nondisaster_trigrams[word] += 1

df_disaster_trigrams = pd.DataFrame(sorted(disaster_trigrams.items(), key=lambda x: x[1])[::-1])
df_nondisaster_trigrams = pd.DataFrame(sorted(nondisaster_trigrams.items(), key=lambda x: x[1])[::-1])

# Plotting Unigrams

fig, axes = plt.subplots(ncols=2, figsize=(6, 6), dpi=100)
plt.tight_layout()

sns.barplot(y=df_disaster_unigrams[0].values[:N], x=df_disaster_unigrams[1].values[:N], ax=axes[0], color='red')
sns.barplot(y=df_nondisaster_unigrams[0].values[:N], x=df_nondisaster_unigrams[1].values[:N], ax=axes[1], color='green')

for i in range(2):
    axes[i].spines['right'].set_visible(False)
    axes[i].set_xlabel('')
    axes[i].set_ylabel('')
    axes[i].tick_params(axis='x', labelsize=8)
    axes[i].tick_params(axis='y', labelsize=8)

axes[0].set_title(f'Top {N} most common unigrams in disaster tweets', fontsize=8)
axes[1].set_title(f'Top {N} most common unigrams in non-disaster tweets', fontsize=8)

plt.show()

# Plotting Bigrams
fig, axes = plt.subplots(ncols=2, figsize=(6, 6), dpi=100)
plt.tight_layout()

sns.barplot(y=df_disaster_bigrams[0].values[:N], x=df_disaster_bigrams[1].values[:N], ax=axes[0], color='red')
sns.barplot(y=df_nondisaster_bigrams[0].values[:N], x=df_nondisaster_bigrams[1].values[:N], ax=axes[1], color='green')

for i in range(2):
    axes[i].spines['right'].set_visible(False)
    axes[i].set_xlabel('')
    axes[i].set_ylabel('')
    axes[i].tick_params(axis='x', labelsize=8)
    axes[i].tick_params(axis='y', labelsize=8)

axes[0].set_title(f'Top {N} most common bigrams in disaster tweets', fontsize=8)
axes[1].set_title(f'Top {N} most common bigrams in non-disaster tweets', fontsize=8)

plt.show()

# Plotting Trigrams
fig, axes = plt.subplots(ncols=2, figsize=(12, 5), dpi=100)

sns.barplot(y=df_disaster_trigrams[0].values[:N], x=df_disaster_trigrams[1].values[:N], ax=axes[0], color='red')
sns.barplot(y=df_nondisaster_trigrams[0].values[:N], x=df_nondisaster_trigrams[1].values[:N], ax=axes[1], color='green')

for i in range(2):
    axes[i].spines['right'].set_visible(False)
    axes[i].set_xlabel('')
    axes[i].set_ylabel('')
    axes[i].tick_params(axis='x', labelsize=6)
    axes[i].tick_params(axis='y', labelsize=6)

axes[0].set_title(f'Top {N} most common trigrams in disaster tweets', fontsize=6)
axes[1].set_title(f'Top {N} most common trigrams in non-disaster tweets', fontsize=6)

plt.show()

!pip install pyspellchecker

# Cleaning and pre-processing including spellchecking

from spellchecker import SpellChecker

spell = SpellChecker()

def correct_spellings_tuned(text):
    words = [word for word in text.split() if len(word) > 3]  # Ignore short words
    misspelled = spell.unknown(words)
    corrections = {word: spell.correction(word) or word for word in misspelled}
    return " ".join([corrections.get(word, word) for word in text.split()])

def correct_spellings(text):
    corrected_text = []
    misspelled_words = spell.unknown(text.split())
    for word in text.split():
        if word in misspelled_words:
          # Check if a correction is found before appending
            correction = spell.correction(word)
            corrected_text.append(correction if correction is not None else word)
        else:
            corrected_text.append(word)
    return " ".join(corrected_text)

# pre-processing text including changing to lower case, removing brackets, punctuation, links, numbers etc.
def clean_text(text):
    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation
    and remove words containing numbers.'''
    text = str(text).lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text

stop_words = stopwords.words('english')
more_stopwords = ['u', 'im', 'c']
stop_words = stop_words + more_stopwords

# removing stopwords that don't provide signal to the target
def remove_stopwords(text):
    text = ' '.join(word for word in text.split(' ') if word not in stop_words)
    return text

stemmer_Snowball = nltk.SnowballStemmer("english")
stemmer_Porter = nltk.stem.PorterStemmer()
lemmatizer_WordNet = nltk.stem.WordNetLemmatizer()

# Stemming the words
def stemm_text(text):
    text = ' '.join(stemmer_Snowball.stem(word) for word in text.split(' '))
    return text

def stemm_text(text):
    text = ' '.join(stemmer_Porter.stem(word) for word in text.split(' '))
    return text

def lemmat_text(text):
    text = ' '.join(lemmatizer_WordNet.lemmatize(word) for word in text.split(' '))
    return text


# Combining all the above functions into a general comprehensive function
def preprocess_data(text):
    # Clean puntuation, urls, and so on
    text = clean_text(text)
    # Spellchecking
    # text = correct_spellings_tuned(text)
    # Remove stopwords
    text = ' '.join(word for word in text.split(' ') if word not in stop_words)
    # Stemm all the words in the sentence
    text = ' '.join(stemmer_Snowball.stem(word) for word in text.split(' '))

    return text

# Pre-processing training data
train.head()
train['text_clean'] = train['text'].apply(preprocess_data)
train.head()

# Pre-processing test data
test.head()
test['text_clean'] = test['text'].apply(preprocess_data)
test.head()

le = LabelEncoder()
le.fit(train['target_relabeled'])
train['target_encoded'] = le.transform(train['target_relabeled'])

# Embeddings

# Embedding 1: Count Vectorizer
random_state_split = 42

def cv(data):
    count_vectorizer = CountVectorizer()

    emb = count_vectorizer.fit_transform(data)

    return emb, count_vectorizer

list_corpus = train["text_clean"].tolist()
list_labels = train["target_encoded"].tolist()

# X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.01, random_state=random_state_split)

X_train_counts, count_vectorizer = cv(list_corpus)
y_train = list_labels

# X_test_counts = count_vectorizer.transform(X_test)

# Visualizing CountVectorizer LSA Embeddings
def plot_LSA(test_data, test_labels, savepath="PCA_demo.csv", plot=True):
        lsa = TruncatedSVD(n_components=2)
        lsa.fit(test_data)
        lsa_scores = lsa.transform(test_data)
        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}
        color_column = [color_mapper[label] for label in test_labels]
        colors = ['orange','blue']
        if plot:
            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))
            orange_patch = mpatches.Patch(color='orange', label='non-disaster')
            blue_patch = mpatches.Patch(color='blue', label='disaster')
            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 10})

fig = plt.figure(figsize=(4, 4))
plot_LSA(X_train_counts, y_train)
plt.show()

# Visualsing TF-IDF LSA Embeddings
def tfidf(data):
    tfidf_vectorizer = TfidfVectorizer()

    train = tfidf_vectorizer.fit_transform(data)

    return train, tfidf_vectorizer

X_train_tfidf, tfidf_vectorizer = tfidf(list_corpus)
# X_test_tfidf = tfidf_vectorizer.transform(X_test)

fig = plt.figure(figsize=(4, 4))
plot_LSA(X_train_tfidf, y_train)
plt.show()

# Visualsing TF-IDF t-SNE Embeddings

def plot_tSNE(test_data, test_labels, savepath="PCA_demo.csv", plot=True):
      tsne = TSNE(
            n_components=2,          # Reduce to 2D
            perplexity=2,            # Adjust based on dataset size (typically 5-50)
            random_state=42,         # Reproducibility
            n_iter=1000,            # Increase for better convergence
            metric="cosine"          # Use cosine distance for text
      )

      # Fit and transform
      embeddings_2d = tsne.fit_transform(test_data)

      color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}
      color_column = [color_mapper[label] for label in test_labels]
      colors = ['orange','blue']
      if plot:
            plt.scatter(embeddings_2d[:,0], embeddings_2d[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))
            orange_patch = mpatches.Patch(color='orange', label='non-disaster')
            blue_patch = mpatches.Patch(color='blue', label='disaster')
            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 10})

fig = plt.figure(figsize=(4, 4))
plot_tSNE(X_train_tfidf, y_train)
plt.show()

# Visualsing Keras Library Tokenizer LSA Embeddings
texts = train['text_clean']
target = train['target_encoded']

# Calculate the length of our vocabulary
word_tokenizer = Tokenizer()
word_tokenizer.fit_on_texts(texts)

vocab_length = len(word_tokenizer.word_index) + 1
print(vocab_length)

def embed(corpus):
    return word_tokenizer.texts_to_sequences(corpus)

longest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))
length_long_sentence = len(word_tokenize(longest_train))

train_padded_sentences = pad_sequences(
    embed(texts),
    length_long_sentence,
    padding='post'
)

train_padded_sentences
fig = plt.figure(figsize=(4, 4))
plot_LSA(train_padded_sentences, target)
plt.show()

# Generating Glove Embeddings

embeddings_dictionary = dict()
embedding_dim = 100

# Load GloVe 100D embeddings
with open('nlp-getting-started/glove-global-vectors-for-word-representation/glove.6B.100d.txt') as fp:
    for line in fp.readlines():
        records = line.split()
        word = records[0]
        vector_dimensions = np.asarray(records[1:], dtype='float32')
        embeddings_dictionary [word] = vector_dimensions

# embeddings_dictionary
# loading embedding vectors of those words that appear in the
# Glove dictionary. Others will be initialized to 0.

embedding_matrix = np.zeros((vocab_length, embedding_dim))

for word, index in word_tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

embedding_matrix

# Visualsing Glove LSA Embeddings
lsa = TruncatedSVD(n_components=2, random_state=42)
embeddings_2d = lsa.fit_transform(embedding_matrix)
words = list(word_tokenizer.word_index.keys())

plt.figure(figsize=(10, 10))
plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5)

# Annotate a subset of words to avoid clutter
for i, word in enumerate(words):
    if i % 50 == 0:  # Label every 10th word (adjust as needed)
        plt.annotate(word,
                     xy=(embeddings_2d[i + 1, 0], embeddings_2d[i + 1, 1]),
                     xytext=(5, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')

plt.title("GloVe Word Embeddings Visualized with LSA")
plt.xlabel("LSA Component 1")
plt.ylabel("LSA Component 2")
plt.grid()
plt.show()

# Visualsing Glove t-SNE Embeddings

from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, random_state=42)
embeddings_2d = tsne.fit_transform(embedding_matrix)
words = list(word_tokenizer.word_index.keys())

plt.figure(figsize=(10, 10))
plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5)

# Annotate a subset of words to avoid clutter
for i, word in enumerate(words):
    if i % 100 == 0:  # Label every 10th word (adjust as needed)
        plt.annotate(word,
                     xy=(embeddings_2d[i + 1, 0], embeddings_2d[i + 1, 1]),
                     xytext=(5, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')

plt.title("GloVe Word Embeddings Visualized with t-SNE")
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")
plt.grid()
plt.show()

pip install umap-learn  # Install the UMAP library

# Visualsing Glove UMAP Embeddings

import umap.umap_ as umap

# Initialize UMAP (adjust hyperparameters as needed)
umap_reducer = umap.UMAP(
    n_components=2,      # Reduce to 2D
    n_neighbors=15,      # Balance local/global structure
    min_dist=0.1,        # Controls cluster tightness (0.0-1.0)
    metric='cosine',     # Use cosine distance for embeddings
    random_state=42      # Reproducibility
)

embeddings_2d = umap_reducer.fit_transform(embedding_matrix)
words = list(word_tokenizer.word_index.keys())

plt.figure(figsize=(8, 8))
plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5)

# Annotate a subset of words to avoid clutter
for i, word in enumerate(words):
    if i % 100 == 0:  # Label every 10th word (adjust as needed)
        plt.annotate(word,
                     xy=(embeddings_2d[i + 1, 0], embeddings_2d[i + 1, 1]),
                     xytext=(5, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')

plt.title("GloVe Word Embeddings Visualized with UMAP")
plt.xlabel("UMAP Component 1")
plt.ylabel("UMAP Component 2")
plt.grid()
plt.show()

# Dependencies
from sklearn.model_selection import train_test_split

import tensorflow

from tqdm import tqdm

from tensorflow.keras.models import Sequential
from tensorflow.keras.initializers import Constant
from tensorflow.keras.layers import (LSTM,
                          Embedding,
                          BatchNormalization,
                          Dense,
                          TimeDistributed,
                          Dropout,
                          Bidirectional,
                          Flatten,
                          GlobalMaxPool1D)
from nltk.tokenize import word_tokenize
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras.utils import plot_model
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import (
    precision_score,
    recall_score,
    f1_score,
    classification_report,
    accuracy_score
)

import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow import keras
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard, Callback, EarlyStopping
from tensorflow.keras.layers import *
from tensorflow.keras import layers,optimizers,models
from keras.metrics import AUC
from tensorflow.keras import backend as KBackend

import psutil
import subprocess
from datetime import datetime

from keras_tuner.tuners import RandomSearch
from keras_tuner.engine.hyperparameters import HyperParameters

import gc
gc.disable()  # Disable garbage collection

# from tensorflow.keras.mixed_precision import set_global_policy
# set_global_policy('mixed_float16')
# tf.debugging.set_log_device_placement(True)  # Log device placement
# tf.debugging.enable_check_numerics()  # Check for NaNs or Infs

# Check if CPU is available
cpu_devices = tf.config.list_physical_devices('CPU')
if cpu_devices:
    print("CPU is available")
else:
    print("CPU is not available")

# Check if GPU is available
gpu_devices = tf.config.list_physical_devices('GPU')
if gpu_devices:
    print("GPU is available")
    for gpu in gpu_devices:
        print(f"GPU device: {gpu}")
else:
    print("GPU is not available")

print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

# viewing train dataset
train.head()

# viewing test dataset
test.head()

df = pd.concat([train, test])
df.shape

def create_corpus_new(df):
    corpus=[]
    for tweet in tqdm(df['text_clean']):
        words=[word.lower() for word in word_tokenize(tweet)]
        corpus.append(words)
    return corpus

corpus = create_corpus_new(df)

embedding_dict={}
with open('nlp-getting-started/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:
    for line in f:
        values=line.split()
        word = values[0]
        vectors=np.asarray(values[1:],'float32')
        embedding_dict[word]=vectors
f.close()

MAX_LEN=50
tokenizer_obj = Tokenizer()
tokenizer_obj.fit_on_texts(corpus)
sequences = tokenizer_obj.texts_to_sequences(corpus)

tweet_pad = pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')

word_index = tokenizer_obj.word_index
print('Number of unique words:',len(word_index))

num_words=len(word_index)+1
embedding_matrix=np.zeros((num_words,100))

for word,i in tqdm(word_index.items()):
    if i < num_words:
        emb_vec=embedding_dict.get(word)
        if emb_vec is not None:
            embedding_matrix[i]=emb_vec

tweet_pad[0][0:]

train_model = tweet_pad[:train.shape[0]]
test_model = tweet_pad[train.shape[0]:]

X_train,X_val,y_train,y_val = train_test_split(train_model,train['target_encoded'].values,test_size=0.2)
print('Shape of train',X_train.shape)
print("Shape of Validation ",X_val.shape)

def build_model(hp):

    model = keras.Sequential()

    # Initializing
    embedding = Embedding(num_words,100,embeddings_initializer = Constant(embedding_matrix),
                   input_length = MAX_LEN,trainable = False)
    # Adding embedding layer
    model.add(embedding)

    # Hyperparameters
    # Spatial dropout rate hyperparameter
    hp_dropout_spatial = hp.Choice('dropout_spatial', values=[0.2, 0.3, 0.4, 0.5])
    model.add(layers.SpatialDropout1D(rate = hp_dropout_spatial))

    # Hyperparameters: LSTM units, Input dropout rate, Recurrent dropout rate
    hp_lstm_units = hp.Int('lstm_units', min_value = 64, max_value = 512, step = 32)
    hp_dropout_recurrent = hp.Choice('dropout_recurrent', values=[0.2, 0.3, 0.4, 0.5])
    hp_dropout_input = hp.Choice('dropout_input', values=[0.2, 0.3, 0.4, 0.5])
    model.add(layers.LSTM(units = hp_lstm_units, dropout= hp_dropout_input, recurrent_dropout = hp_dropout_recurrent, return_sequences = True))

    # Maxpooling for reduction in dimension
    model.add(GlobalMaxPool1D())

    # Regularization
    model.add(BatchNormalization())

    # Hyperparameter: Dropout rate in dense layers
    hp_dropout_dense = hp.Choice('dropout_dense', values=[0.2, 0.3, 0.4, 0.5])
    # Hyperparameter: Number of Dense layers and units in dense layers
    for i in range(hp.Int('num_dense_layers', 1, 3)):
        model.add(layers.Dense(units=hp.Int(f'dense_units_{i}', min_value=4, max_value=256, step=32), activation='relu'))
        model.add(layers.Dropout(rate=hp_dropout_dense))

    # Output layer
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(1, activation='sigmoid'))

    # Hyperparameter: Learning rate
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
    optimzer = Adam(learning_rate = hp_learning_rate)

    model.compile(loss='binary_crossentropy',optimizer = optimzer,metrics=['accuracy'])

    return model

tuner = RandomSearch(
    build_model,
    objective='val_loss',  # Metric to optimize
    max_trials=30,             # Number of hyperparameter combinations to try
    executions_per_trial=1,    # Number of models to train per trial (for robustness)
    directory='nlp-getting-started/my_tuner_dir',  # Directory to save results
    project_name='lstm_tuning'  # Project name
)

tuner.search(
    X_train, y_train,                  # Training data (NumPy arrays)
    epochs=2,                         # More epochs for better tuning
    validation_data=(X_val, y_val),     # Validation data
    batch_size=256,
    verbose = 1,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(patience=3)  # Stop if no improvement
      # tf.keras.callbacks.TensorBoard('./logs')       # Logs for visualization
    ]
)

best_hp_model = tuner.get_best_models(num_models=1)[0]

# Print the summary
best_hp_model.summary()

# Get the best hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

# Print the best hyperparameters
print(f"""
Best hyperparameters:
- Dropout Spatial Rate: {best_hps.get('dropout_spatial')}
- LSTM Units: {best_hps.get('lstm_units')}
- Dropout Recurrent: {best_hps.get('dropout_recurrent')}
- Dropout Input: {best_hps.get('dropout_input')}
- Dense Layers: {best_hps.get('num_dense_layers')}
- Dense Units Layer 0: {best_hps.get('dense_units_0')}
- Dense Units Layer 1: {best_hps.get('dense_units_1')}
- Dense Units Layer 2: {best_hps.get('dense_units_2')}
- Dense Units Layer 3: {best_hps.get('dense_units_3')}
- Learning Rate: {best_hps.get('learning_rate')}
- Dropout Dense: {best_hps.get('dropout_dense')}
""")

# Helper functions for training models and logging
# Define the checkpoint callback
checkpoint_path = "nlp-getting-started/models/lstm_bidirectional_model/model_checkpoint_epoch_{epoch:02d}.h5"

# Define the checkpoint callback
checkpoint_epoch_save = ModelCheckpoint(
    checkpoint_path,
    save_freq='epoch',
    save_weights_only=False,
    verbose=1
)

checkpoint_learning_rate = ReduceLROnPlateau(
            monitor='val_loss',
            mode='min',          # Reduce LR when val_loss stops decreasing
            factor=0.1,  # Reduce learning rate by a factor of 10
            patience=3,  # Wait for 3 epochs without improvement
            min_lr=1e-7, # Minimum learning rate
            verbose=1
)

early_stopping = EarlyStopping(
    monitor='val_loss',  # Monitor validation loss
    mode='min',          # Stop training when val_loss stops decreasing
    patience=5,          # Stop after 5 epochs without improvement
    min_delta=0.001,     # Minimum change to qualify as improvement
    restore_best_weights=False  # Restore the best model weights
)

# Create a log directory for TensorBoard
log = "nlp-getting-started/logs/profile/" + datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = TensorBoard(log_dir=log, profile_batch='10,20')


class ResourceUsageCallback(Callback):
    def on_epoch_end(self, epoch, logs=None):
        # Log GPU usage
        result = subprocess.run(['rocm-smi'], stdout=subprocess.PIPE)
        print("GPU Usage:")
        print(result.stdout.decode('utf-8'))

        # Log CPU and memory usage
        cpu_percent = psutil.cpu_percent(interval=1)
        memory_info = psutil.virtual_memory()
        print(f"CPU Usage: {cpu_percent}%")
        print(f"Memory Usage: {memory_info.percent}%")

# Add the callback to model.fit
resource_callback = ResourceUsageCallback()


# Flag to track profiler state
profiler_running = False

def start_profiler(logdir):
    global profiler_running
    tf.profiler.experimental.start(logdir)
    profiler_running = True
    print("Profiler started.")

def stop_profiler():
    global profiler_running
    if profiler_running:
        tf.profiler.experimental.stop()
        profiler_running = False
        print("Profiler stopped.")
    else:
        print("Profiler is not running.")

def base_model():

    model = keras.Sequential()

    # Initializing
    embedding = Embedding(num_words,100,embeddings_initializer = Constant(embedding_matrix),
                   input_length = MAX_LEN,trainable = False)

    # Adding embedding layer
    model.add(embedding)

    # Spatial dropout rate hyperparameter
    model.add(layers.SpatialDropout1D(rate = 0.5))

    # Hyperparameters: LSTM units, Input dropout rate, Recurrent dropout rate
    model.add(layers.LSTM(units = 256, dropout= 0.2, recurrent_dropout = 0.4, return_sequences = True))

    # Maxpooling for reduction in dimension
    model.add(GlobalMaxPool1D())

    # Regularization
    model.add(BatchNormalization())

    # Hyperparameter: Number of Dense layers and units in dense layers
    model.add(layers.Dense(units = 128, activation='relu'))
    model.add(layers.Dropout(rate=0.2))

    model.add(layers.Dense(units = 64, activation='relu'))
    model.add(layers.Dropout(rate=0.2))

    model.add(layers.Dense(units = 32, activation='relu'))
    model.add(layers.Dropout(rate=0.2))

    # Output layer
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(1, activation='sigmoid'))

    return model

base_model_lstm = base_model()
opt = tf.keras.optimizers.Adam(0.0001)
base_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])
base_model_lstm.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# stop_profiler()
# KBackend.clear_session()
# # Start the profiler
# # start_profiler('histopathologic-cancer-detection/logdir')
# # Training from epoch 1st to epoch 7th
base_model_lstm_history = base_model_lstm.fit(
     X_train,y_train,
     batch_size=256,
     epochs = 20,
     validation_data = (X_val,y_val),
     verbose = 1,
     callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
)
# 
# # Check and stop the profiler
# # stop_profiler()
# 
# # Clear the session

# Finetuning the model
# KBackend.set_value(cnn_model.optimizer.learning_rate, 0.00001)
# Load the model from a specific epoch
# Training from epoch 8th to epoch 17th
base_model_lstm = load_model("nlp-getting-started/models/base_model_lstm/model_checkpoint_epoch_07.h5")
opt = tf.keras.optimizers.Adam(0.0001)
base_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

base_model_lstm_history_1 = base_model_lstm.fit(
    X_train,y_train,
    batch_size=256,
    epochs = 13,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
)

# Finetuning the model
# KBackend.set_value(cnn_model.optimizer.learning_rate, 0.00001)
# Load the model from a specific epoch
# Training from epoch 18th to epoch 27th
base_model_lstm = load_model("nlp-getting-started/models/base_model_lstm/model_checkpoint_epoch_17.h5")
opt = tf.keras.optimizers.Adam(0.0001)
base_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

base_model_lstm_history_2 = base_model_lstm.fit(
    X_train,y_train,
    batch_size=256,
    epochs = 10,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
  )

# Finetuning the model
# KBackend.set_value(cnn_model.optimizer.learning_rate, 0.00001)
# Load the model from a specific epoch

# Training from epoch 28th to epoch 35th
base_model_lstm = load_model("nlp-getting-started/models/base_model_lstm/model_checkpoint_epoch_27.h5")
opt = tf.keras.optimizers.Adam(0.001)
base_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

base_model_lstm_history_3 = base_model_lstm.fit(
    X_train,y_train,
    batch_size=256,
    epochs = 23,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
  )

# Finetuning the model
# KBackend.set_value(cnn_model.optimizer.learning_rate, 0.00001)
# Load the model from a specific epoch
# Training from epoch 36th to epoch 37th
base_model_lstm = load_model("nlp-getting-started/models/base_model_lstm/model_checkpoint_epoch_35.h5")
opt = tf.keras.optimizers.Adam(0.0001)
base_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

base_model_lstm_history_4 = base_model_lstm.fit(
    X_train,y_train,
    batch_size=256,
    initial_epoch = 35,
    epochs = 50,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
  )

# Training from epoch 38th to epoch 41st
base_model_lstm = load_model("nlp-getting-started/models/base_model_lstm/model_checkpoint_epoch_37.h5")
opt = tf.keras.optimizers.Adam(0.0001)
base_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

base_model_lstm_history_5 = base_model_lstm.fit(
    X_train,y_train,
    batch_size=256,
    initial_epoch = 37,
    epochs = 50,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
  )

# Training from epoch 42nd to epoch 45th
base_model_lstm = load_model("nlp-getting-started/models/base_model_lstm/model_checkpoint_epoch_41.h5")
opt = tf.keras.optimizers.Adam(0.00001)
base_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

base_model_lstm_history_6 = base_model_lstm.fit(
    X_train,y_train,
    batch_size=256,
    initial_epoch = 41,
    epochs = 50,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
  )

# Performance metrics for base model training with Adam optimizer
base_model_loss = [
    # First set of epochs (7)
    0.7809, 0.7203, 0.6877, 0.6619, 0.6406, 0.6132, 0.6213,
    # Second set of epochs (10)
    0.5990, 0.5818, 0.5766, 0.5728, 0.5653, 0.5642, 0.5528, 0.5643, 0.5368, 0.5494,
    # Third set of epochs (10)
    0.5498, 0.5479, 0.5466, 0.5421, 0.5296, 0.5307, 0.5362, 0.5313, 0.5233, 0.5320,
    # Fourth set of epochs (8 from Epoch 1/23 to 8/23)
    0.5326, 0.5322, 0.5199, 0.5090, 0.5107, 0.4990, 0.4945, 0.4951,
    # Fifth set of epochs (2 from Epoch 36/50 to 37/50)
    0.4810, 0.4796,
    # Sixth set of epochs (38/50 to 41/50)
    0.4877, 0.4778, 0.4748, 0.4775,
    # Seventh set of epochs (42/50 to 45/50)
    0.4815, 0.4718, 0.4756, 0.4711

]

base_model_val_loss = [
    # First set of epochs (7)
    0.6782, 0.6669, 0.6585, 0.6515, 0.6449, 0.6388, 0.6338,
    # Second set of epochs (10)
    0.6356, 0.6289, 0.6213, 0.6126, 0.6048, 0.5969, 0.5879, 0.5794, 0.5716, 0.5636,
    # Third set of epochs (10)
    0.5583, 0.5552, 0.5484, 0.5429, 0.5392, 0.5347, 0.5316, 0.5308, 0.5259, 0.5261,
    # Fourth set of epochs (8 from Epoch 1/23 to 8/23)
    0.5014, 0.4873, 0.4773, 0.4726, 0.4650, 0.4669, 0.4650, 0.4692,
    # Fifth set of epochs (2 from Epoch 36/50 to 37/50)
    0.4642, 0.4631,
    # Sixth set of epochs (38/50 to 41/50)
    0.4457, 0.4484, 0.4487, 0.4489,
    # Seventh set of epochs (42/50 to 45/50)
    0.4492, 0.4500, 0.4500, 0.4503
]

base_model_accuracy = [
    # First 7 epochs
    0.5414, 0.5829, 0.6141, 0.6360, 0.6637, 0.6833, 0.6791,
    # Next 10 epochs
    0.6911, 0.7077, 0.7133, 0.7149, 0.7292, 0.7268, 0.7358, 0.7253, 0.7433, 0.7427,
    # Next 10 epochs
    0.7358, 0.7358, 0.7453, 0.7429, 0.7504, 0.7486, 0.7422, 0.7493, 0.7524, 0.7493,
    # Next 8 epochs (Epoch 1/23 to 8/23)
    0.7470, 0.7429, 0.7557, 0.7639, 0.7631, 0.7709, 0.7736, 0.7677,
    # Final 2 epochs (Epoch 36/50 to 37/50)
    0.7844, 0.7826,
    # Sixth set of epochs (38/50 to 41/50)
    0.7768, 0.7818, 0.7783, 0.7811,
    # Seventh set of epochs (42/50 to 45/50)
    0.7801, 0.7865, 0.7847, 0.7844
]

base_model_val_accuracy = [
    # First 7 epochs
    0.6264, 0.6087, 0.6021, 0.6172, 0.6323, 0.6461, 0.6540,
    # Next 10 epochs
    0.6507, 0.6612, 0.6691, 0.6802, 0.6875, 0.6914, 0.7052, 0.7124, 0.7163, 0.7196,
    # Next 10 epochs
    0.7216, 0.7223, 0.7269, 0.7295, 0.7308, 0.7360, 0.7374, 0.7354, 0.7387, 0.7380,
    # Next 8 epochs (Epoch 1/23 to 8/23)
    0.7492, 0.7597, 0.7768, 0.7781, 0.7833, 0.7800, 0.7820, 0.7768,
    # Final 2 epochs (Epoch 36/50 to 37/50)
    0.7814, 0.7833,
    # Sixth set of epochs (38/50 to 41/50)
    0.7945, 0.7919, 0.7912, 0.7912,
    # Seventh set of epochs (42/50 to 45/50)
    0.7919, 0.7919, 0.7925, 0.7919
]

base_model_lstm_RMSProp = base_model()
opt = tf.keras.optimizers.RMSprop(0.001)
base_model_lstm_RMSProp.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# stop_profiler()
# KBackend.clear_session()
# # Start the profiler
# # start_profiler('histopathologic-cancer-detection/logdir')
# Training from epoch 1st to epoch 10th
 base_model_lstm_RMSProp_history = base_model_lstm_RMSProp.fit(
     X_train,y_train,
     batch_size=256,
     epochs = 50,
     validation_data = (X_val,y_val),
     verbose = 1,
     callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
 )
# 
# # Check and stop the profiler
# # stop_profiler()
# 
# # Clear the session

# Training from epoch 11th to epoch 20th
base_model_lstm_RMSProp = load_model("nlp-getting-started/models/base_model_lstm_RMSProp/model_checkpoint_epoch_10.h5")
opt = tf.keras.optimizers.RMSprop(0.001)
base_model_lstm_RMSProp.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

base_model_lstm_RMSProp_history_2 = base_model_lstm_RMSProp.fit(
    X_train,y_train,
    batch_size=256,
    initial_epoch = 10,
    epochs = 50,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
)

# Training from epoch 21st to epoch 30th
base_model_lstm_RMSProp = load_model("nlp-getting-started/models/base_model_lstm_RMSProp/model_checkpoint_epoch_20.h5")
opt = tf.keras.optimizers.RMSprop(0.0001)
base_model_lstm_RMSProp.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

base_model_lstm_RMSProp_history_2 = base_model_lstm_RMSProp.fit(
    X_train,y_train,
    batch_size=256,
    initial_epoch = 20,
    epochs = 50,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
)

# Training from epoch 31st to epoch 36th
base_model_lstm_RMSProp = load_model("nlp-getting-started/models/base_model_lstm_RMSProp/model_checkpoint_epoch_30.h5")
opt = tf.keras.optimizers.RMSprop(0.00001)
base_model_lstm_RMSProp.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

base_model_lstm_RMSProp_history_2 = base_model_lstm_RMSProp.fit(
    X_train,y_train,
    batch_size=256,
    initial_epoch = 30,
    epochs = 50,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
  )

# Performance metrics for base model trained with RMSProp optimizer
base_model_RMSProp_training_loss = [
    0.6511, 0.5732, 0.5435, 0.5357, 0.5290,
    0.5182, 0.5126, 0.5005, 0.4929, 0.4922,
    0.4951, 0.4768, 0.4746, 0.4580, 0.4596,
    0.4549, 0.4420, 0.4388, 0.4318, 0.4347,
    0.4413, 0.4309, 0.4300, 0.4299, 0.4317,
    0.4291, 0.4251, 0.4255, 0.4245, 0.4219,
    0.4229, 0.4249, 0.4206, 0.4239, 0.4216,
    0.4212
]

base_model_RMSProp_validation_loss = [
    0.6435, 0.6176, 0.6056, 0.5912, 0.5878,
    0.5768, 0.5670, 0.5604, 0.5472, 0.5406,
    0.5186, 0.5129, 0.4975, 0.4824, 0.4854,
    0.4793, 0.4639, 0.4756, 0.4605, 0.4711,
    0.4026, 0.3977, 0.3930, 0.3950, 0.3927,
    0.3898, 0.3897, 0.3884, 0.3868, 0.3897,
    0.3931, 0.3927, 0.3925, 0.3923, 0.3927,
    0.3924
]

base_model_RMSProp_training_accuracy = [
    0.6565, 0.7222, 0.7448, 0.7491, 0.7494,
    0.7617, 0.7596, 0.7639, 0.7704, 0.7750,
    0.7706, 0.7823, 0.7814, 0.7993, 0.7931,
    0.7977, 0.8015, 0.8018, 0.8080, 0.8061,
    0.8013, 0.8097, 0.8115, 0.8133, 0.8044,
    0.8122, 0.8113, 0.8095, 0.8136, 0.8179,
    0.8131, 0.8079, 0.8138, 0.8163, 0.8156,
    0.8131
]

base_model_RMSProp_validation_accuracy = [
    0.6842, 0.7012, 0.6999, 0.7393, 0.7249,
    0.7360, 0.7380, 0.7603, 0.7617, 0.7761,
    0.7754, 0.7866, 0.7892, 0.7859, 0.7879,
    0.7859, 0.7873, 0.7774, 0.7853, 0.7853,
    0.8273, 0.8280, 0.8299, 0.8280, 0.8280,
    0.8299, 0.8299, 0.8319, 0.8319, 0.8293,
    0.8306, 0.8313, 0.8313, 0.8313, 0.8306,
    0.8306
]

def impr_model():

    model = keras.Sequential()

    # Initializing
    embedding = Embedding(num_words,100,embeddings_initializer = Constant(embedding_matrix),
                   input_length = MAX_LEN,trainable = False)

    # Adding embedding layer
    model.add(embedding)

    # Spatial dropout rate hyperparameter
    model.add(layers.SpatialDropout1D(rate = 0.5))

    # Hyperparameters: LSTM units, Input dropout rate, Recurrent dropout rate
    model.add(layers.Bidirectional(LSTM(units = 512, dropout= 0.2, recurrent_dropout = 0.4, return_sequences = True)))

    # Maxpooling for reduction in dimension
    model.add(GlobalMaxPool1D())

    # Regularization
    model.add(BatchNormalization())

    # Hyperparameter: Number of Dense layers and units in dense layers
    model.add(layers.Dense(units = 256, activation='relu'))
    model.add(layers.Dropout(rate=0.2))

    model.add(layers.Dense(units = 128, activation='relu'))
    model.add(layers.Dropout(rate=0.2))

    model.add(layers.Dense(units = 64, activation='relu'))
    model.add(layers.Dropout(rate=0.2))

    model.add(layers.Dense(units = 32, activation='relu'))
    model.add(layers.Dropout(rate=0.2))

    # Output layer
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(1, activation='sigmoid'))

    return model

impr_model_lstm = impr_model()
opt = tf.keras.optimizers.RMSprop(0.001)
impr_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])
impr_model_lstm.summary()

# Commented out IPython magic to ensure Python compatibility.
# # Load the model and train!!
# %%time
# stop_profiler()
# KBackend.clear_session()
# # Start the profiler
# # start_profiler('histopathologic-cancer-detection/logdir')
# 
impr_model_lstm_history = impr_model_lstm.fit(
     X_train,y_train,
     batch_size=256,
     epochs = 50,
     validation_data = (X_val,y_val),
     verbose = 1,
     callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
 )
# 
# # Check and stop the profiler
# # stop_profiler()
# 
# # Clear the session

# Training from epoch 5th to epoch 8th
impr_model_lstm = load_model("nlp-getting-started/models/lstm_bidirectional_model/model_checkpoint_epoch_04.h5")
opt = tf.keras.optimizers.RMSprop(0.001)
impr_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

impr_model_lstm_history_1 = impr_model_lstm.fit(
    X_train,y_train,
    batch_size=256,
    initial_epoch = 4,
    epochs = 50,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
  )

# Training from epoch 9th to epoch 12th
impr_model_lstm = load_model("nlp-getting-started/models/lstm_bidirectional_model/model_checkpoint_epoch_08.h5")
opt = tf.keras.optimizers.RMSprop(0.0001)
impr_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

impr_model_lstm_history_2 = impr_model_lstm.fit(
    X_train,y_train,
    batch_size=256,
    initial_epoch = 8,
    epochs = 50,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
  )

# Training from epoch 13th to epoch 16th
impr_model_lstm = load_model("nlp-getting-started/models/lstm_bidirectional_model/model_checkpoint_epoch_12.h5")
opt = tf.keras.optimizers.RMSprop(0.0001)
impr_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

impr_model_lstm_history_3 = impr_model_lstm.fit(
    X_train,y_train,
    batch_size=256,
    initial_epoch = 12,
    epochs = 50,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
  )

# Training from epoch 17th to epoch 20th
impr_model_lstm = load_model("nlp-getting-started/models/lstm_bidirectional_model/model_checkpoint_epoch_16.h5")
opt = tf.keras.optimizers.RMSprop(0.0001)
impr_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

impr_model_lstm_history_4 = impr_model_lstm.fit(
    X_train,y_train,
    batch_size=256,
    initial_epoch = 16,
    epochs = 50,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
  )

# Training from epoch 21st to epoch 24th
impr_model_lstm = load_model("nlp-getting-started/models/lstm_bidirectional_model/model_checkpoint_epoch_20.h5")
opt = tf.keras.optimizers.RMSprop(0.0001)
impr_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

impr_model_lstm_history_5 = impr_model_lstm.fit(
    X_train,y_train,
    batch_size=256,
    initial_epoch = 20,
    epochs = 50,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]
  )

# Training from epoch 25th to epoch 28th
impr_model_lstm = load_model("nlp-getting-started/models/lstm_bidirectional_model/model_checkpoint_epoch_24.h5")
opt = tf.keras.optimizers.RMSprop(0.00001)
impr_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

impr_model_lstm_history_6 = impr_model_lstm.fit(
    X_train,y_train,
    batch_size=256,
    initial_epoch = 24,
    epochs = 50,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]

  )

# Training from epoch 29th to epoch 32nd
impr_model_lstm = load_model("nlp-getting-started/models/lstm_bidirectional_model/model_checkpoint_epoch_28.h5")
opt = tf.keras.optimizers.RMSprop(0.00001)
impr_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

impr_model_lstm_history_7 = impr_model_lstm.fit(
    X_train,y_train,
    batch_size=256,
    initial_epoch = 28,
    epochs = 50,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]

  )

# Training from epoch 33rd to epoch 36th
impr_model_lstm = load_model("nlp-getting-started/models/lstm_bidirectional_model/model_checkpoint_epoch_32.h5")
opt = tf.keras.optimizers.RMSprop(0.00001)
impr_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

impr_model_lstm_history_8 = impr_model_lstm.fit(
    X_train,y_train,
    batch_size=256,
    initial_epoch = 32,
    epochs = 50,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]

  )

# Training from epoch 37th to epoch 40th
impr_model_lstm = load_model("nlp-getting-started/models/lstm_bidirectional_model/model_checkpoint_epoch_36.h5")
opt = tf.keras.optimizers.RMSprop(0.000001)
impr_model_lstm.compile(loss='binary_crossentropy',optimizer = opt, metrics=['accuracy'])

impr_model_lstm_history_9 = impr_model_lstm.fit(
    X_train,y_train,
    batch_size=256,
    initial_epoch = 36,
    epochs = 50,
    validation_data = (X_val,y_val),
    verbose = 1,
    callbacks=[checkpoint_epoch_save, checkpoint_learning_rate, early_stopping]

  )

# Improved Bidirectional LSTM model performance metrics

impr_training_loss = [
    0.6381, 0.5628, 0.5369, 0.5292, 0.5262, 0.5142, 0.5013, 0.4858, 0.4817,
    0.4732, 0.4727, 0.4641, 0.4695, 0.4602, 0.4628, 0.4585, 0.4617, 0.4557,
    0.4506, 0.4477, 0.4427, 0.4362, 0.4410, 0.4347, 0.4437, 0.4375, 0.4404,
    0.4431, 0.4390, 0.4311, 0.4356, 0.4367, 0.4413, 0.4304, 0.4301, 0.4231,
    0.4360, 0.4293, 0.4300, 0.4317
]

impr_validation_loss = [
    0.6285, 0.6079, 0.5937, 0.6087, 0.5807, 0.5858, 0.5729, 0.5640, 0.5509,
    0.5467, 0.5404, 0.5315, 0.5071, 0.5018, 0.4934, 0.4840, 0.4799, 0.4715,
    0.4654, 0.4555, 0.4529, 0.4474, 0.4429, 0.4401, 0.4167, 0.4152, 0.4134,
    0.4122, 0.4196, 0.4190, 0.4186, 0.4182, 0.4391, 0.4391, 0.4397, 0.4399,
    0.4351, 0.4350, 0.4348, 0.4347
]

impr_accuracy = [
    0.6690, 0.7304, 0.7430, 0.7573, 0.7568, 0.7655, 0.7693, 0.7818, 0.7778,
    0.7874, 0.7875, 0.7897, 0.7849, 0.7893, 0.7892, 0.7892, 0.7921, 0.7900,
    0.7972, 0.7990, 0.7995, 0.8067, 0.8021, 0.8064, 0.8016, 0.8026, 0.7980,
    0.7997, 0.8003, 0.8062, 0.8046, 0.7990, 0.8036, 0.8084, 0.8062, 0.8077,
    0.8054, 0.8102, 0.8066, 0.8043
]

impr_validation_accuracy = [
    0.7787, 0.7748, 0.7800, 0.7085, 0.7420, 0.7295, 0.7485, 0.7511, 0.7689,
    0.7682, 0.7708, 0.7695, 0.7925, 0.7912, 0.7932, 0.7945, 0.8050, 0.8056,
    0.8056, 0.8050, 0.8109, 0.8102, 0.8122, 0.8122, 0.8227, 0.8214, 0.8221,
    0.8234, 0.8129, 0.8122, 0.8122, 0.8116, 0.8004, 0.8011, 0.8004, 0.8011,
    0.8030, 0.8030, 0.8030, 0.8030
]

# Helper functions for visualizing the performance metrics
def merge_history(hlist):
    history = {}
    for k in hlist[0].history.keys():
        history[k] = sum([h.history[k] for h in hlist], [])
    return history

def vis_training(h, start=1):
    epoch_range = range(start, len(h['loss'])+1)
    s = slice(start-1, None)

    plt.figure(figsize=[14,4])

    n = int(len(h.keys()) / 2)

    for i in range(n):
        k = list(h.keys())[i]
        plt.subplot(1,n,i+1)
        plt.plot(epoch_range, h[k][s], label='Training')
        plt.plot(epoch_range, h['val_' + k][s], label='Validation')
        plt.xlabel('Epoch'); plt.ylabel(k); plt.title(k)
        plt.grid()
        plt.legend()

    plt.tight_layout()
    plt.show()

def plot_training_history(train_loss, val_loss, train_acc, val_acc):
    epochs = list(range(1, len(train_loss) + 1))

    plt.figure(figsize=(12, 6))

    # Loss plot
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_loss, label='Train Loss', marker='o')
    plt.plot(epochs, val_loss, label='Validation Loss', marker='x')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True)

    # Accuracy plot
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_acc, label='Train Accuracy', marker='o')
    plt.plot(epochs, val_acc, label='Validation Accuracy', marker='x')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

def metrics(pred_tag, y_test):

    # Convert predictions to binary labels using a threshold (e.g., 0.5)
    pred_tag = (pred_tag > 0.5).astype(int)
    print("F1-score: ", round(f1_score(pred_tag, y_test), 2))
    print("Precision: ", round(precision_score(pred_tag, y_test), 2))
    print("Recall: ", round(recall_score(pred_tag, y_test), 2))
    print("Acuracy: ", round(accuracy_score(pred_tag, y_test), 2))
    print("-"*50)
    print(classification_report(pred_tag, y_test))

def plot(history, arr):
    fig, ax = plt.subplots(1, 2, figsize=(20, 5))
    for idx in range(2):
        ax[idx].plot(history.history[arr[idx][0]])
        ax[idx].plot(history.history[arr[idx][1]])
        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)
        ax[idx].set_xlabel('A ',fontsize=16)
        ax[idx].set_ylabel('B',fontsize=16)
        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)

plot_training_history(base_model_loss, base_model_val_loss, base_model_accuracy, base_model_val_accuracy)

model = load_model("nlp-getting-started/models/base_model_lstm/model_checkpoint_epoch_38.h5")
y_pred = model.predict(X_val)
metrics(y_pred, y_val)

plot_training_history(base_model_RMSProp_training_loss, base_model_RMSProp_validation_loss, base_model_RMSProp_training_accuracy, base_model_RMSProp_validation_accuracy)

model = load_model("nlp-getting-started/models/base_model_lstm_RMSProp/model_checkpoint_epoch_28.h5")
y_pred = model.predict(X_val)
metrics(y_pred, y_val)

plot_training_history(impr_training_loss, impr_validation_loss, impr_accuracy, impr_validation_accuracy)

model = load_model("nlp-getting-started/models/lstm_bidirectional_model/model_checkpoint_epoch_28.h5")
y_pred = model.predict(X_val)
metrics(y_pred, y_val)

# Choosing the best model with highest F1 score
model = load_model("nlp-getting-started/models/base_model_lstm_RMSProp/model_checkpoint_epoch_28.h5")
test_pred = model.predict(test_model)

submission.target = test_pred.round().astype('int') # Converting the probabilities into target labels: 0 or 1
submission.to_csv("submission.csv", index=False)
submission.target.value_counts().plot.bar();

submission
